{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b53a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.preprocessor import Preprocessor\n",
    "from representation.representation import Representation\n",
    "from machine_learning.classification import Classification\n",
    "from evaluation.classifier_evaluation import Evaluation\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799f2bf",
   "metadata": {},
   "source": [
    "#### Lendo conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85a3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/imdb-reviews-pt-br.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976c117",
   "metadata": {},
   "source": [
    "#### Pegando 1500 amostras do conjunto de dados original de forma balanceada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d100eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegando apenas os textos que possuem sentimento negativo\n",
    "df_negative_sentiment = dataset.loc[dataset['sentiment'] == 'neg']\n",
    "# pegando apenas os textos que possuem sentimento positivo\n",
    "df_positive_sentiment = dataset.loc[dataset['sentiment'] == 'pos']\n",
    "\n",
    "# # criando um conjunto de dados novo com apenas 1500 amostras, tendo um balanceamento de 750 amostras positivas e 750 amostras negativas\n",
    "df = pd.concat([df_negative_sentiment[0:1500], df_positive_sentiment[0:1500]])\n",
    "# embaralha o novo conjunto de dados\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a5fe089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id', 'text_en'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f642357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A princípio, parece que o romance temático ond...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>É sempre difícil trazer um livro de 450 página...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algum estranho esquisito que tinha três famíli...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Em sua superfície, este é um dos filmes de açã...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Às vezes você consegue exatamente o que espera...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_pt sentiment\n",
       "0  A princípio, parece que o romance temático ond...       pos\n",
       "1  É sempre difícil trazer um livro de 450 página...       pos\n",
       "2  Algum estranho esquisito que tinha três famíli...       neg\n",
       "3  Em sua superfície, este é um dos filmes de açã...       pos\n",
       "4  Às vezes você consegue exatamente o que espera...       neg"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.drop('index', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e194b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6753f7",
   "metadata": {},
   "source": [
    "### Aplicando One-Hot-Enconding\n",
    "- Vai transforma variaveis categoricas em variaveis discretas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c74436c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A princípio, parece que o romance temático ond...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>É sempre difícil trazer um livro de 450 página...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algum estranho esquisito que tinha três famíli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Em sua superfície, este é um dos filmes de açã...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Às vezes você consegue exatamente o que espera...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_pt  sentiment\n",
       "0  A princípio, parece que o romance temático ond...          1\n",
       "1  É sempre difícil trazer um livro de 450 página...          1\n",
       "2  Algum estranho esquisito que tinha três famíli...          0\n",
       "3  Em sua superfície, este é um dos filmes de açã...          1\n",
       "4  Às vezes você consegue exatamente o que espera...          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = pd.get_dummies(df['sentiment'], prefix='sentiment', drop_first=True)\n",
    "df['sentiment'] = sentiment\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0b546c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Algum estranho esquisito que tinha três famílias, traiu e negligenciou todos eles, construiu edifícios inúteis feios por toda parte que agora são desvalorizados e desmoronando. Seu filho bastardo, meio judeu, anda por aí entrevistando aleatoriamente pessoas senis que não nos interessam e mostra suas terríveis habilidades de narrar e escrever enquanto trágicas músicas de piano tocam. Isso se prolonga por quase duas horas chatas e não dá em nada. Todas as pessoas hippies superficiais que assistem a esses documentários estúpidos, comendo saladas e iogurtes, acham que toda essa porcaria é tão importante. Não é. Salve as baleias. Ninguém se importa.'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_pt\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1db18d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aplicando preprocessamento no texto para remover sujeiras de dentro do conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f61e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas de pre processamento\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "478c5208",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_words(words):\n",
    "    word = re.sub('([\\.\\, \\\\/ \\\\\" \\\\^\\\\\\'\\_\\´\\`\\’\\@\\#\\$\\%\\?\\!\\:\\;\\*-\\=\\+\\{\\}]+)', '', words)\n",
    "    word = re.sub('/\\.', '', word)\n",
    "    word = re.sub('(etc|[0-9]{1,4}|decada|seculo|a{2,200}|a{2,200}h{2,200}|a{2,200}rgh|-{1,200})+', '', word)\n",
    "    return word\n",
    "\n",
    "def clean_sentences(text: List[str]) -> List[str]:\n",
    "    alpha = [' ','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    remove_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "    text = re.sub('[!\"#$%&()*\\+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n]', '', text)\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    text = text.lower()  # change to lower case\n",
    "    text = ''.join([char for char in text if char in alpha])\n",
    "    text = ' '.join([clean_words(word) for word in text.split(' ') if\n",
    "                     ((word not in remove_stopwords) & (len(word) > 1))])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5552a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text_pt\"] = df[\"text_pt\"].apply(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c1e9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Texto original vs Texto Pre-processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5bedc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original\n",
      "\n",
      " A princípio, parece que o romance temático onde uma garota conhece um garoto e se apaixona, mas o ponto é que esse filme tem um sentimento que outros não têm. Na primeira vez que o vi, não consegui concluir, porque tinha que sair. Mas enquanto eu estava andando eu pensei que eu deveria ver isso de novo, mas eu não tive qualquer oportunidade até então. Um ano se passou até que eu vi este filme em um canal não-livre e eu vi e eu gravei também. uma vez, duas vezes ... até 200 vezes e não está brincando.Eu sabia todos os diálogos por hart e eu não sei por que, mas eu vi todos os dias e nunca se cansar.E eu tenho que dizer que eu não estou acostumado a ver um filme mais do que duas vezes. O ato é muito bom. Gerard Depardieu é um ator talentoso e katherine heigl também. Eu gostaria que ela estivesse em um bom filme, porque eu acho que ela pode fazê-lo.Em equilíbrio, é um filme que eu não posso tirar da minha mente. \n",
      "\n",
      "Qtd de palavras =  922\n"
     ]
    }
   ],
   "source": [
    "print(\"Texto original\\n\\n\", df['text_pt'].iloc[0], \"\\n\\nQtd de palavras = \", len(df['text_pt'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e1966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto pre-processado\n",
      "\n",
      " principio parece romance tematico onde garota conhece garoto apaixona ponto filme sentimento outros nao primeira vez vi nao consegui concluir porque sair enquanto andando pensei deveria ver novo nao qualquer oportunidade ate entao ano passou ate vi filme canal naolivre vi gravei tambem vez duas vezes ate vezes nao brincandoeu sabia todos dialogos hart nao sei vi todos dias nunca cansare dizer nao acostumado ver filme duas vezes ato bom gerard depardieu ator talentoso katherine heigl tambem gostaria bom filme porque acho pode fazeloem equilibrio filme nao posso tirar mente \n",
      "\n",
      "Qtd de palavras =  578\n"
     ]
    }
   ],
   "source": [
    "print(\"Texto pre-processado\\n\\n\", df['clean_text_pt'].iloc[0], \"\\n\\nQtd de palavras = \", len(df['clean_text_pt'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2307fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Criando uma analise pelo tamanho do texto original vs o tamanho do texto pre-processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c176c0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text_pt</th>\n",
       "      <th>length</th>\n",
       "      <th>clean_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A princípio, parece que o romance temático ond...</td>\n",
       "      <td>1</td>\n",
       "      <td>principio parece romance tematico onde garota ...</td>\n",
       "      <td>922</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>É sempre difícil trazer um livro de 450 página...</td>\n",
       "      <td>1</td>\n",
       "      <td>sempre dificil trazer livro paginas filme tres...</td>\n",
       "      <td>1760</td>\n",
       "      <td>1298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algum estranho esquisito que tinha três famíli...</td>\n",
       "      <td>0</td>\n",
       "      <td>algum estranho esquisito tres familias traiu n...</td>\n",
       "      <td>652</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Em sua superfície, este é um dos filmes de açã...</td>\n",
       "      <td>1</td>\n",
       "      <td>superficie filmes acao comedia romance classic...</td>\n",
       "      <td>782</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Às vezes você consegue exatamente o que espera...</td>\n",
       "      <td>0</td>\n",
       "      <td>vezes voce consegue exatamente espera filme pr...</td>\n",
       "      <td>763</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_pt  sentiment  \\\n",
       "0  A princípio, parece que o romance temático ond...          1   \n",
       "1  É sempre difícil trazer um livro de 450 página...          1   \n",
       "2  Algum estranho esquisito que tinha três famíli...          0   \n",
       "3  Em sua superfície, este é um dos filmes de açã...          1   \n",
       "4  Às vezes você consegue exatamente o que espera...          0   \n",
       "\n",
       "                                       clean_text_pt  length  clean_length  \n",
       "0  principio parece romance tematico onde garota ...     922           578  \n",
       "1  sempre dificil trazer livro paginas filme tres...    1760          1298  \n",
       "2  algum estranho esquisito tres familias traiu n...     652           525  \n",
       "3  superficie filmes acao comedia romance classic...     782           543  \n",
       "4  vezes voce consegue exatamente espera filme pr...     763           551  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df.text_pt.str.len()\n",
    "df['clean_length'] = df.clean_text_pt.str.len()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4cfc600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprimento original: 3772674\n",
      "Comprimento limpo: 2720330\n",
      "Total de palavras removidas: 1052344\n"
     ]
    }
   ],
   "source": [
    "# Remoção de comprimento total\n",
    "print (\"Comprimento original:\", df.length.sum ())\n",
    "print (\"Comprimento limpo:\", df.clean_length.sum ())\n",
    "print (\"Total de palavras removidas:\", (df.length.sum ()) - (df.clean_length.sum ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8d658c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentagem de dados removidos =  27.89384929628163\n"
     ]
    }
   ],
   "source": [
    "# Porcentagem da quantidade de dados indesejados limpo ou processado após a remoção do comprimento.\n",
    "print(\"Porcentagem de dados removidos = \", (1052344 / 3772674) * 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2ab6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Aplicando EDA para entender o comportamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612e097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sns.countplot(x=df.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48ac5d7",
   "metadata": {},
   "source": [
    "### Divisão do conjunto de dados em 70/30 Test-Train\n",
    "- train = 70% dos dados\n",
    "- test = 30% dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cd7835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df[\"clean_text_pt\"]\n",
    "Y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f61f4f",
   "metadata": {},
   "source": [
    "### Aplicando a representacao TF-IDF\n",
    "- Representando as palavras dentro do espaco vetorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f94196fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b83b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a540a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2100, n_features: 30169\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % X_train_vectors_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cb299482",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa2cb8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 900, n_features: 30169\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % X_test_vectors_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0025edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 30169)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectors_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2de658dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1e502",
   "metadata": {},
   "source": [
    "### Contruindo o modelo de Naive Bayes\n",
    "- Utilizando os dados de traino e teste representados na forma de vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "794f018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "313f589a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_classifier = GaussianNB()\n",
    "naive_bayes_classifier.fit(X_train_vectors_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "388b6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = naive_bayes_classifier.predict(X_test_vectors_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96e37b",
   "metadata": {},
   "source": [
    "### Avaliação dos modelos\n",
    "\n",
    "Um resumo sobre as métricas avaliativas\n",
    "\n",
    "- accuracy ou acuracia esta função calcula a precisão do subconjunto: o conjunto de rótulos previsto para uma amostra deve corresponder exatamente ao conjunto correspondente de rótulos em y_true.\n",
    "\n",
    "- A precision ou precisão é a proporção em que está o número de verdadeiros positivos e o número de falsos positivos. A precisão é intuitivamente a capacidade do classificador de não rotular como positiva uma amostra negativa. O melhor valor é 1 e o pior valor é 0.\n",
    "\n",
    "- O recall ou cobertura é a proporção em que está o número de verdadeiros positivos e o número de falsos negativos. O recall é intuitivamente a capacidade do classificador de encontrar todas as amostras positivas. O melhor valor é 1 e o pior valor é 0.\n",
    "\n",
    "- A pontuação F1 pode ser interpretada como uma média ponderada da precisão e recuperação, onde uma pontuação F1 atinge seu melhor valor em 1 e a pior pontuação em 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b1f07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1104bce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.83      0.82      0.83       437\n",
      "    Negative       0.83      0.84      0.84       463\n",
      "\n",
      "    accuracy                           0.83       900\n",
      "   macro avg       0.83      0.83      0.83       900\n",
      "weighted avg       0.83      0.83      0.83       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['Positive', 'Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8c10f6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[360  77]\n",
      " [ 74 389]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a26f0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFazendo analise de um comentario do filme Harry Potter, dessa forma iremos ver o que o algoritmo treinado ira nos retorna\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fazendo analise de um comentario do filme Harry Potter, dessa forma iremos ver o que o algoritmo treinado ira nos retorna\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c95f7930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAlgum estranho esquisito que tinha três famílias, traiu e negligenciou todos eles, construiu edifícios inúteis feios por toda parte que agora são desvalorizados e desmoronando. Seu filho bastardo, meio judeu, anda por aí entrevistando aleatoriamente pessoas senis que não nos interessam e mostra suas terríveis habilidades de narrar e escrever enquanto trágicas músicas de piano tocam. Isso se prolonga por quase duas horas chatas e não dá em nada. Todas as pessoas hippies superficiais que assistem a esses documentários estúpidos, comendo saladas e iogurtes, acham que toda essa porcaria é tão importante. Não é. Salve as baleias. Ninguém se importa.\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Algum estranho esquisito que tinha três famílias, traiu e negligenciou todos eles, construiu edifícios inúteis feios por toda parte que agora são desvalorizados e desmoronando. Seu filho bastardo, meio judeu, anda por aí entrevistando aleatoriamente pessoas senis que não nos interessam e mostra suas terríveis habilidades de narrar e escrever enquanto trágicas músicas de piano tocam. Isso se prolonga por quase duas horas chatas e não dá em nada. Todas as pessoas hippies superficiais que assistem a esses documentários estúpidos, comendo saladas e iogurtes, acham que toda essa porcaria é tão importante. Não é. Salve as baleias. Ninguém se importa.\n",
    "\"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d090510",
   "metadata": {},
   "source": [
    "- Observacao, para cada entrada de dados nova a ser analisada pelo algoritmo ela deve passar pelos seguintes passos\n",
    "    - Pre-processamento\n",
    "    - Representacao vetorial do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9aeb9f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'algum estranho esquisito tres familias traiu negligenciou todos construiu edificios inuteis feios toda parte agora sao desvalorizados desmoronando filho bastardo meio judeu anda ai entrevistando aleatoriamente pessoas senis nao interessam mostra terriveis habilidades narrar escrever enquanto tragicas musicas piano tocam prolonga quase duas horas chatas nao nada todas pessoas hippies superficiais assistem documentarios estupidos comendo saladas iogurtes acham toda porcaria tao importante nao salve baleias ninguem importa'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processed = clean_sentences(text)\n",
    "text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0ec31149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30169)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = tf_idf.transform([text_processed]).toarray()\n",
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "48479f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliacao Negativa\n"
     ]
    }
   ],
   "source": [
    "#0= avaliacao negativa\n",
    "#1= avaliacao positiva\n",
    "result = naive_bayes_classifier.predict(test_input)[0]\n",
    "if result==1:\n",
    "    print(\"Avaliacao Positiva\")\n",
    "elif result==0:\n",
    "    print(\"Avaliacao Negativa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e526443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (movie-reviews-pt-br)",
   "language": "python",
   "name": "pycharm-f1288bc7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
